{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Application to train networks to play pong against each other\nThis program makes two spiking neural networks of two layers each compete\nagainst each other in encoding an input-output mapping within their weights.\n\nThe script is centered around a simulation of the classic game Pong\nin which the vertical position of the ball determines the input for both\n'players' and their output activation after a predetermined simulation\ntime changes the paddle positions within the game.\n\nThe output of the script stores information about the state of the game,\nand both networks after every simulation step. this data is saved to three\n.pkl files after the simulation is complete, which can be used to visualize\nthe output (e.g., using :doc:`generate_gif.py <generate_gif>`).\n\nThe idea for this simulation as well as the core of the R-STDP and Pong\nimplementation are from [1]_ and were created by Timo Wunderlich and Electronic\nVision(s) (The original implementation can be found\n[here](https://github.com/electronicvisions/model-sw-pong)).\nThe visualization and implementation of dopaminergic learning, as well as\nchanges to the existing codebase were developed by Johannes Gille (2022).\n\n# See Also\n:doc:`Code for visualizing the output <generate_gif>`\n\n\n# References\n.. [1] Wunderlich T., et al (2019). Demonstrating advantages of\n       neuromorphic computation: a pilot study. Frontiers in neuroscience, 13,\n       260. https://doi.org/10.3389/fnins.2019.00260\n\n:Authors: J Gille, T Wunderlich, Electronic Vision(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport datetime\nimport gzip\nimport logging\nimport nest\nimport os\nimport sys\nimport time\n\nimport numpy as np\nimport pickle\n\nimport pong\nfrom networks import POLL_TIME, PongNetDopa, PongNetRSTDP\n\n\nclass AIPong:\n    def __init__(self, p1, p2, out_dir=\"\"):\n        \"\"\"A class to run and store pong simulations of two competing spiking\n        neural networks.\n\n        Args:\n            p1 (PongNet): Network to play on the left side.\n            p2 (PongNet): Network to play on the right side.\n            out_folder (str, optional): Name of the output folder. Defaults to\n            current time stamp (YYYY-mm-dd-HH-MM-SS).\n        \"\"\"\n        self.game = pong.GameOfPong()\n        self.player1 = p1\n        self.player2 = p2\n\n        if out_dir == \"\":\n            out_dir = '{0:%Y-%m-%d-%H-%M-%S}'.format(datetime.datetime.now())\n        if os.path.exists(out_dir):\n            print(f\"output folder {out_dir} already exists!\")\n            sys.exit()\n        os.mkdir(out_dir)\n        self.out_dir = out_dir\n\n        logging.info(f\"setup complete for a pong game between: {p1} and {p2}.\")\n\n    def run_games(self, max_runs=10000):\n        \"\"\"Runs a simulation of pong games and stores the results.\n\n        Args:\n            max_runs (int, optional): Number of iterations to simulate.\n            Defaults to 10000.\n        \"\"\"\n        self.game_data = []\n        l_score, r_score = 0, 0\n\n        start_time = time.time()\n        self.run = 0\n        biological_time = 0\n\n        logging.info(f\"Starting simulation of {max_runs} iterations of \"\n                     f\"{POLL_TIME}ms each.\")\n        while self.run < max_runs:\n            logging.debug(\"\")\n            logging.debug(f\"Iteration {self.run}:\")\n            self.input_index = self.game.ball.get_cell()[1]\n            self.player1.set_input_spiketrain(\n                self.input_index, biological_time)\n            self.player2.set_input_spiketrain(\n                self.input_index, biological_time)\n\n            if self.run % 100 == 0:\n                logging.info(\n                    f\"{round(time.time() - start_time, 2)}: Run \"\n                    f\"{self.run}, score: {l_score, r_score}, mean rewards: \"\n                    f\"{round(np.mean(self.player1.mean_reward), 3)}, \"\n                    f\"{round(np.mean(self.player2.mean_reward), 3)}\")\n\n            logging.debug(\"Running simulation...\")\n            nest.Simulate(POLL_TIME)\n            biological_time = nest.GetKernelStatus(\"biological_time\")\n\n            for network, paddle in zip(\n                    [self.player1, self.player2],\n                    [self.game.l_paddle, self.game.r_paddle]):\n\n                network.apply_synaptic_plasticity(biological_time)\n                network.reset()\n\n                position_diff = network.winning_neuron - paddle.get_cell()[1]\n                if position_diff > 0:\n                    paddle.move_up()\n                elif position_diff == 0:\n                    paddle.dont_move()\n                else:\n                    paddle.move_down()\n\n            self.game.step()\n            self.run += 1\n            self.game_data.append(\n                [self.game.ball.get_pos(),\n                 self.game.l_paddle.get_pos(),\n                 self.game.r_paddle.get_pos(),\n                 (l_score, r_score)])\n\n            if self.game.result == pong.RIGHT_SCORE:\n                self.game.reset_ball(False)\n                r_score += 1\n            elif self.game.result == pong.LEFT_SCORE:\n                self.game.reset_ball(True)\n                l_score += 1\n\n        end_time = time.time()\n        logging.info(\n            f\"Simulation of {max_runs} runs complete after: \"\n            f\"{datetime.timedelta(seconds=end_time - start_time)}\")\n\n        self.game_data = np.array(self.game_data)\n\n        out_data = dict()\n        out_data[\"ball_pos\"] = self.game_data[:, 0]\n        out_data[\"left_paddle\"] = self.game_data[:, 1]\n        out_data[\"right_paddle\"] = self.game_data[:, 2]\n        out_data[\"score\"] = self.game_data[:, 3]\n\n        logging.info(\"saving game data...\")\n        with open(os.path.join(self.out_dir, \"gamestate.pkl\"), \"wb\") as file:\n            pickle.dump(out_data, file)\n\n        logging.info(\"saving network data...\")\n\n        for net, filename in zip([self.player1, self.player2],\n                                 [\"data_left.pkl.gz\", \"data_right.pkl.gz\"]):\n            with gzip.open(os.path.join(self.out_dir, filename), \"w\") as file:\n                output = {\"network_type\": repr(net),\n                          \"with_noise\": net.apply_noise}\n                performance_data = net.get_performance_data()\n                output[\"rewards\"] = performance_data[0]\n                output[\"weights\"] = performance_data[1]\n                pickle.dump(output, file)\n\n        logging.info(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    nest.set_verbosity(\"M_WARNING\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--runs\",\n                        type=int,\n                        default=5000,\n                        help=\"Number of game steps to simulate.\")\n    parser.add_argument(\"--debug\",\n                        action=\"store_true\",\n                        help=\"Verbose debugging output.\")\n    parser.add_argument(\"--out_dir\",\n                        type=str,\n                        default=\"\",\n                        help=\"Directory to save experiments to. Defaults to \\\n                        current time stamp (YYYY-mm-dd-HH-MM-SS)\")\n    parser.add_argument(\n        \"--players\", nargs=2, type=str, choices=[\"r\", \"rn\", \"d\", \"dn\"],\n        default=[\"r\", \"rn\"],\n        help=\"\"\"Types of networks that compete against each other. four learning\n        rule configuations are available: r:  r-STDP without noise, rn: r-STDP\n        with noisy input, d:  dopaminergic synapses without noise,\n        dn: dopaminergic synapses with noisy input.\"\"\")\n\n    args, unknown = parser.parse_known_args()\n\n    level = logging.DEBUG if args.debug else logging.INFO\n    format = '%(asctime)s - %(message)s'\n    datefmt = '%H:%M:%S'\n    logging.basicConfig(level=level, format=format, datefmt=datefmt)\n\n    p1, p2 = args.players\n    if p1[0] == p2[0] == 'd':\n        logging.error(\"\"\"Nest currently (August 2022) does not support\n        addressing multiple populations of dopaminergic synapses because all of\n        them recieve their signal from a single volume transmitter. For this\n        reason, no two dopaminergic networks can be trained simultaneously. One\n        of the players needs to be changed to the R-STDP type.\"\"\")\n        sys.exit()\n\n    apply_noise = len(p1) > 1\n    if p1[0] == \"r\":\n        p1 = PongNetRSTDP(apply_noise)\n    else:\n        p1 = PongNetDopa(apply_noise)\n\n    apply_noise = len(p2) > 1\n    if p2[0] == \"r\":\n        p2 = PongNetRSTDP(apply_noise)\n    else:\n        p2 = PongNetDopa(apply_noise)\n\n    AIPong(p1, p2, args.out_dir).run_games(max_runs=args.runs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
     "display_name": "EBRAINS-22.10",
     "language": "python",
     "name": "spack_python_kernel_release_202210"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
